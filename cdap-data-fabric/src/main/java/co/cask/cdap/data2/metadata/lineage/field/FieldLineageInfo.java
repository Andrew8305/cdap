/*
 * Copyright Â© 2018 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package co.cask.cdap.data2.metadata.lineage.field;

import co.cask.cdap.api.lineage.field.EndPoint;
import co.cask.cdap.api.lineage.field.InputField;
import co.cask.cdap.api.lineage.field.Operation;
import co.cask.cdap.api.lineage.field.OperationType;
import co.cask.cdap.api.lineage.field.ReadOperation;
import co.cask.cdap.api.lineage.field.TransformOperation;
import co.cask.cdap.api.lineage.field.WriteOperation;
import co.cask.cdap.proto.codec.OperationTypeAdapter;
import com.google.common.base.Charsets;
import com.google.common.collect.Sets;
import com.google.gson.Gson;
import com.google.gson.GsonBuilder;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.stream.Collectors;

/**
 * Class representing the information about field lineage for a single program run.
 * Currently we store the operations associated with the field lineage and corresponding
 * checksum. Algorithm to compute checksum is same as how Avro computes the Schema fingerprint.
 * (https://issues.apache.org/jira/browse/AVRO-1006). The implementation of fingerprint
 * algorithm is taken from {@code org.apache.avro.SchemaNormalization} class. Since the checksum
 * is persisted in store, any change to the canonicalize form or fingerprint algorithm would
 * require upgrade step to update the stored checksums.
 */
public class FieldLineageInfo {
  private static final Gson GSON = new GsonBuilder()
    .registerTypeAdapter(Operation.class, new OperationTypeAdapter())
    .create();

  private final Set<Operation> operations;

  // Map of EndPoint representing destination to the set of fields belonging to it.
  private Map<EndPoint, Set<String>> destinationFields;

  // For each (EndPoint,Field) for the destination we maintain the incoming summary i.e.
  // combination of (EndPoint,Field) which were responsible for generating it.
  private Map<EndPointField, Set<EndPointField>> incomingSummary;

  // For each (EndPoint,Field) combination from the source, we maintain the outgoing summary i.e.
  // set of (EndPoint, Field)s which were generated by it.
  private Map<EndPointField, Set<EndPointField>> outgoingSummary;

  private transient Set<ReadOperation> readOperations;
  private transient Set<WriteOperation> writeOperations;

  // Map of operation name to the operation
  private transient Map<String, Operation> operationsMap;

  private transient Map<String, Set<String>> operationOutgoingConnections;

  // Source endpoints in the lineage info
  private transient Set<EndPoint> sources;

  // Destination endpoints in the lineage info
  private transient Set<EndPoint> destinations;

  private long checksum;

  /**
   * Create an instance of a class from supplied collection of operations.
   * Validations are performed on the collection before creating instance. All of the operations
   * must have unique names. Collection must have at least one operation of type READ and one
   * operation of type WRITE. The origins specified for the {@link InputField} are also validated
   * to make sure the operation with the corresponding name exists in the collection. However, we do not
   * validate the existence of path to the fields in the destination from sources. If no such path exists
   * then the lineage will be incomplete.
   *
   * @param operations the collection of field lineage operations
   * @throws IllegalArgumentException if validation fails
   */
  public FieldLineageInfo(Collection<? extends Operation> operations) {
    this.operations = new HashSet<>(operations);
    computeAndValidateFieldLineageInfo(operations);
    this.checksum = computeChecksum();
  }

  private void computeAndValidateFieldLineageInfo(Collection<? extends Operation> operations) {
    Set<String> allOrigins = new HashSet<>();

    this.operationsMap = new HashMap<>();
    this.readOperations = new HashSet<>();
    this.writeOperations = new HashSet<>();
    this.operationOutgoingConnections = new HashMap<>();

    for (Operation operation : operations) {
      if (operationsMap.containsKey(operation.getName())) {
        throw new IllegalArgumentException(String.format("All operations provided for creating field " +
                "level lineage info must have unique names. " +
                "Operation name '%s' is repeated.", operation.getName()));

      }

      operationsMap.put(operation.getName(), operation);

      switch (operation.getType()) {
        case READ:
          ReadOperation read = (ReadOperation) operation;
          EndPoint source = read.getSource();
          if (source == null) {
            throw new IllegalArgumentException(String.format("Source endpoint cannot be null for the read " +
                    "operation '%s'.", read.getName()));
          }
          readOperations.add(read);
          break;
        case TRANSFORM:
          TransformOperation transform = (TransformOperation) operation;
          Set<String> origins = transform.getInputs().stream().map(InputField::getOrigin).collect(Collectors.toSet());
          // for each origin corresponding to the input fields there is a connection from that origin to this operation
          for (String origin : origins) {
            Set<String> connections = operationOutgoingConnections.computeIfAbsent(origin, k -> new HashSet<>());
            connections.add(transform.getName());
          }
          allOrigins.addAll(origins);
          break;
        case WRITE:
          WriteOperation write = (WriteOperation) operation;
          EndPoint destination = write.getDestination();
          if (destination == null) {
            throw new IllegalArgumentException(String.format("Destination endpoint cannot be null for the write " +
                    "operation '%s'.", write.getName()));
          }

          origins = write.getInputs().stream().map(InputField::getOrigin).collect(Collectors.toSet());
          // for each origin corresponding to the input fields there is a connection from that origin to this operation
          for (String origin : origins) {
            Set<String> connections = operationOutgoingConnections.computeIfAbsent(origin, k -> new HashSet<>());
            connections.add(write.getName());
          }
          allOrigins.addAll(origins);
          writeOperations.add(write);
          break;
        default:
          // no-op
      }
    }

    Set<String> operationsWithNoOutgoingConnections
            = Sets.difference(operationsMap.keySet(), operationOutgoingConnections.keySet());
    // put empty set for operations with no outgoing connection rather than checking for null later
    for (String operation : operationsWithNoOutgoingConnections) {
      operationOutgoingConnections.put(operation, new HashSet<>());
    }

    if (readOperations.isEmpty()) {
      throw new IllegalArgumentException("Field level lineage requires at least one operation of type 'READ'.");
    }

    if (writeOperations.isEmpty()) {
      throw new IllegalArgumentException("Field level lineage requires at least one operation of type 'WRITE'.");
    }

    Sets.SetView<String> invalidOrigins = Sets.difference(allOrigins, operationsMap.keySet());
    if (!invalidOrigins.isEmpty()) {
      throw new IllegalArgumentException(String.format("No operation is associated with the origins '%s'.",
              invalidOrigins));
    }
  }

  /**
   * @return the checksum for the operations
   */
  public long getChecksum() {
    return checksum;
  }

  /**
   * @return the operations
   */
  public Set<Operation> getOperations() {
    return operations;
  }

  /**
   * @return the map of destination EndPoint's and corresponding fields those were written to them
   */
  public Map<EndPoint, Set<String>> getDestinationFields() {
    if (destinationFields == null) {
      destinationFields = computeDestinationFields();
    }
    return destinationFields;
  }

  public Map<EndPointField, Set<EndPointField>> getIncomingSummary() {
    if (incomingSummary == null) {
      incomingSummary = computeIncomingSummary();
    }
    return incomingSummary;
  }

  public Map<EndPointField, Set<EndPointField>> getOutgoingSummary() {
    if (outgoingSummary == null) {
      outgoingSummary = computeOutgoingSummary();
    }
    return outgoingSummary;
  }

  /**
   * @return all {@link EndPoint}s representing the source for read operations
   */
  public Set<EndPoint> getSources() {
    if (sources == null) {
      populateSourcesAndDestinations();
    }
    return sources;
  }

  /**
   * @return all {@link EndPoint}s representing the destination for write operations
   */
  public Set<EndPoint> getDestinations() {
    if (destinations == null) {
      populateSourcesAndDestinations();
    }
    return destinations;
  }

  private void populateSourcesAndDestinations() {
    sources = new HashSet<>();
    destinations = new HashSet<>();
    for (Operation operation : operations) {
      if (OperationType.READ == operation.getType()) {
        ReadOperation read = (ReadOperation) operation;
        sources.add(read.getSource());
      } else if (OperationType.WRITE == operation.getType()) {
        WriteOperation write = (WriteOperation) operation;
        destinations.add(write.getDestination());
      }
    }
  }

  private long computeChecksum() {
    return fingerprint64(canonicalize().getBytes(Charsets.UTF_8));
  }

  private Map<EndPoint, Set<String>> computeDestinationFields() {
    if (writeOperations == null) {
      computeAndValidateFieldLineageInfo(this.operations);
    }

    Map<EndPoint, Set<String>> destinationFields = new HashMap<>();
    for (WriteOperation write : this.writeOperations) {
      Set<String> endPointFields = destinationFields.computeIfAbsent(write.getDestination(), k -> new HashSet<>());
      for (InputField field : write.getInputs()) {
        endPointFields.add(field.getName());
      }
    }
    return destinationFields;
  }

  private Map<EndPointField, Set<EndPointField>> computeIncomingSummary() {
    if (writeOperations == null) {
      computeAndValidateFieldLineageInfo(this.operations);
    }

    Map<EndPointField, Set<EndPointField>> result = new HashMap<>();
    for (WriteOperation write : writeOperations) {
      List<InputField> inputs = write.getInputs();
      for (InputField input : inputs) {
        Set<String> visitedOperationNames = new HashSet<>();
        computeIncomingSummaryHelper(new EndPointField(write.getDestination(), input.getName()),
                                     operationsMap.get(input.getOrigin()), write, visitedOperationNames, result);
      }
    }
    return result;
  }

  private void computeIncomingSummaryHelper(EndPointField destination, Operation currentOperation,
                                            Operation previousOperation, Set<String> visitedOperationNames,
                                            Map<EndPointField, Set<EndPointField>> result) {
    if (!visitedOperationNames.add(currentOperation.getName())) {
      // cycles detected
      // TODO: CDAP-13548
    }

    if (currentOperation.getType() == OperationType.READ) {
      // if current operation is of type READ, previous operation must be of type TRANSFORM or WRITE
      // get only the input fields from the previous operations for which the origin is current READ operation
      Set<InputField> inputFields = new HashSet<>();
      if (OperationType.WRITE == previousOperation.getType()) {
        WriteOperation previousWrite = (WriteOperation) previousOperation;
        inputFields = new HashSet<>(previousWrite.getInputs());
      } else if (OperationType.TRANSFORM == previousOperation.getType()) {
        TransformOperation previousTransform = (TransformOperation) previousOperation;
        inputFields = new HashSet<>(previousTransform.getInputs());
      }

      Set<EndPointField> sourceEndPointFields = result.computeIfAbsent(destination, k -> new HashSet<>());

      ReadOperation read = (ReadOperation) currentOperation;
      EndPoint source = read.getSource();
      for (InputField inputField : inputFields) {
        if (inputField.getOrigin().equals(currentOperation.getName())) {
          sourceEndPointFields.add(new EndPointField(source, inputField.getName()));
        }
      }

      return;
    }

    if (currentOperation.getType() != OperationType.TRANSFORM) {
      return;
    }

    TransformOperation transform = (TransformOperation) currentOperation;
    for (InputField field : transform.getInputs()) {
      computeIncomingSummaryHelper(destination, operationsMap.get(field.getOrigin()), currentOperation,
                                   visitedOperationNames, result);
    }
  }

  private Map<EndPointField, Set<EndPointField>> computeOutgoingSummary() {
    if (incomingSummary == null) {
      incomingSummary = computeIncomingSummary();
    }

    Map<EndPointField, Set<EndPointField>> outgoingSummary = new HashMap<>();
    for (Map.Entry<EndPointField, Set<EndPointField>> entry : incomingSummary.entrySet()) {
      Set<EndPointField> values = entry.getValue();
      for (EndPointField value : values) {
        Set<EndPointField> outgoingEndPointFields = outgoingSummary.computeIfAbsent(value, k -> new HashSet<>());
        outgoingEndPointFields.add(entry.getKey());
      }
    }
    return outgoingSummary;
  }

  /**
   * Get the set of outgoing operations from a given field of an EndPoint.
   * Since returned operations only contain the projection relevant for the given field,
   * we need to create new operations containing these projections. For example, if the
   * original read operation creates fields 'offset' and 'body' and request is for getting the
   * outgoing operations for 'body', the 'offset' field will be eliminated from the output of read.
   *
   * @param sourceField the field belonging to the EndPoint for which outgoing operations to be returned
   * @return outgoing operations
   */
  public Set<Operation> getOutgoingOperationsFromField(EndPointField sourceField) {
    if (readOperations == null) {
      // make sure non-transient fields are computed
      computeAndValidateFieldLineageInfo(this.operations);
    }

    Map<String, Operation> result = new HashMap<>();
    for (ReadOperation read : readOperations) {
      if (read.getSource().equals(sourceField.getEndPoint()) && read.getOutputs().contains(sourceField.getField())) {
        ReadOperation readToAdd = new ReadOperation(read.getName(), read.getDescription(), sourceField.getEndPoint(),
                                                    sourceField.getField());
        result.put(readToAdd.getName(), readToAdd);
        Set<String> operationsToProcess = new HashSet<>(operationOutgoingConnections.get(read.getName()));
        for (String operationToProcess : operationsToProcess) {
          outgoingOperationsFromFieldHelper(operationToProcess, readToAdd, result);
        }
      }
    }
    return new HashSet<>(result.values());
  }

  private void outgoingOperationsFromFieldHelper(String currentOperationNameToProcess, Operation previousOperation,
                                                 Map<String, Operation> result) {

    Operation currentOperation = operationsMap.get(currentOperationNameToProcess);

    List<String> commonFields = commonFieldsBetweenOperations(previousOperation, currentOperation);
    if (commonFields.isEmpty()) {
      return;
    }

    List<InputField> inputFields = new ArrayList<>();
    for (String field : commonFields) {
      inputFields.add(InputField.of(previousOperation.getName(), field));
    }

    Operation operationToAdd = null;

    if (currentOperation.getType() == OperationType.TRANSFORM) {
      TransformOperation transform = (TransformOperation) currentOperation;
      operationToAdd = new TransformOperation(transform.getName(), transform.getDescription(), inputFields,
                                              transform.getOutputs());
    } else if (currentOperation.getType() == OperationType.WRITE) {
      WriteOperation write = (WriteOperation) currentOperation;
      operationToAdd = new WriteOperation(write.getName(), write.getDescription(), write.getDestination(),
              inputFields);
    }

    if (operationToAdd != null) {
      Operation operation = result.get(operationToAdd.getName());
      if (operation != null) {
        // operation was already added as a part of another path
        // create new operation after merging the input and output fields of both
        operationToAdd = mergeOperationInputOutputs(operation, operationToAdd);
      }
      result.put(operationToAdd.getName(), operationToAdd);
    }

    Set<String> connections = operationOutgoingConnections.get(currentOperation.getName());
    for (String connection : connections) {
      outgoingOperationsFromFieldHelper(connection, currentOperation, result);
    }
  }

  private Operation mergeOperationInputOutputs(Operation first, Operation second) {
    Operation result = null;
    switch (first.getType()) {
      case READ:
        ReadOperation firstRead = (ReadOperation) first;
        ReadOperation secondRead = (ReadOperation) second;
        List<String> outputs = new ArrayList<>();
        outputs.addAll(firstRead.getOutputs());
        outputs.addAll(secondRead.getOutputs());
        result = new ReadOperation(first.getName(), first.getDescription(), firstRead.getSource(), outputs);
        break;
      case TRANSFORM:
        TransformOperation firstTransform = (TransformOperation) first;
        TransformOperation secondTransform = (TransformOperation) second;
        List<InputField> inputs = new ArrayList<>();
        inputs.addAll(firstTransform.getInputs());
        inputs.addAll(secondTransform.getInputs());
        outputs = new ArrayList<>();
        outputs.addAll(firstTransform.getOutputs());
        outputs.addAll(secondTransform.getOutputs());
        result = new TransformOperation(firstTransform.getName(), firstTransform.getDescription(), inputs, outputs);
        break;
      case WRITE:
        WriteOperation firstWrite = (WriteOperation) first;
        WriteOperation secondWrite = (WriteOperation) second;
        inputs = new ArrayList<>();
        inputs.addAll(firstWrite.getInputs());
        inputs.addAll(secondWrite.getInputs());
        result = new WriteOperation(firstWrite.getName(), firstWrite.getDescription(), firstWrite.getDestination(),
                                  inputs);
    }
    return result;
  }

  private List<String> commonFieldsBetweenOperations(Operation from, Operation to) {
    List<String> outputs = new ArrayList<>();
    if (from.getType() == OperationType.READ) {
      ReadOperation read = (ReadOperation) from;
      outputs.addAll(read.getOutputs());
    } else if (from.getType() == OperationType.TRANSFORM) {
      TransformOperation transform = (TransformOperation) from;
      outputs.addAll(transform.getOutputs());
    }

    List<InputField> inputFields = new ArrayList<>();
    if (to.getType() == OperationType.TRANSFORM) {
      TransformOperation transform = (TransformOperation) to;
      inputFields.addAll(transform.getInputs());
    } else if (to.getType() == OperationType.WRITE) {
      WriteOperation write = (WriteOperation) to;
      inputFields.addAll(write.getInputs());
    }

    List<String> result = new ArrayList<>();
    for (InputField inputField : inputFields) {
      if (inputField.getOrigin().equals(from.getName()) && outputs.contains(inputField.getName())) {
        result.add(inputField.getName());
      }
    }

    return result;
  }

  public Set<Operation> getIncomingOperationsForField(EndPointField destinationField) {
    if (writeOperations == null) {
      computeAndValidateFieldLineageInfo(this.operations);
    }

    Map<String, Operation> result = new HashMap<>();
    for (WriteOperation write : writeOperations) {
      if (write.getDestination().equals(destinationField.getEndPoint())) {
        List<InputField> inputFields = write.getInputs();
        InputField inputFieldForDestinationField = null;
        for (InputField inputField : inputFields) {
          if (inputField.getName().equals(destinationField.getField())) {
            inputFieldForDestinationField = inputField;
            // inputs of write operations should have unique field names, irrespective of the origins
            // TODO: Add validations for this case - if input field names are not unique this means merge
            // operation before write is missing
            break;
          }
        }

        WriteOperation writeToAdd = new WriteOperation(write.getName(), write.getDescription(), write.getDestination(),
                                                       inputFieldForDestinationField);
        result.put(writeToAdd.getName(), writeToAdd);
        if (inputFieldForDestinationField != null) {
          incomingOperationsForFieldHelper(inputFieldForDestinationField.getOrigin(), writeToAdd, result);
        }
      }
    }
    return new HashSet<>(result.values());
  }

  private void incomingOperationsForFieldHelper(String currentOperationName, Operation previousOperation,
                                                Map<String, Operation> result) {

    Operation currentOperation = operationsMap.get(currentOperationName);

    List<String> commonFields = commonFieldsBetweenOperations(currentOperation, previousOperation);
    if (commonFields.isEmpty()) {
      return;
    }

    Operation operationToAdd = null;
    Set<String> operationsToProcess = new HashSet<>();
    if (currentOperation.getType() == OperationType.TRANSFORM) {
      TransformOperation transform = (TransformOperation) currentOperation;
      operationToAdd = new TransformOperation(transform.getName(), transform.getDescription(), transform.getInputs(),
                                              commonFields.toArray(new String[0]));

      List<InputField> newInputsToProcess = new ArrayList<>(transform.getInputs());
      operationsToProcess.addAll(newInputsToProcess.stream().map(InputField::getOrigin).collect(Collectors.toSet()));

    } else if (currentOperation.getType() == OperationType.READ) {
      ReadOperation read = (ReadOperation) currentOperation;
      operationToAdd = new ReadOperation(read.getName(), read.getDescription(), read.getSource(),
                                         commonFields.toArray(new String[0]));
    }

    if (operationToAdd != null) {
      Operation operation = result.get(operationToAdd.getName());
      if (operation != null) {
        // operation was already added as a part of another path
        // create new operation after merging the input and output fields of both
        operationToAdd = mergeOperationInputOutputs(operation, operationToAdd);
      }
      result.put(operationToAdd.getName(), operationToAdd);
    }

    for (String operationToProcess : operationsToProcess) {
      incomingOperationsForFieldHelper(operationToProcess, currentOperation, result);
    }
  }

  /**
   * Creates the canonicalize representation of the collection of operations. Canonicalize representation is
   * simply the JSON format of operations. Before creating the JSON, collection of operations is sorted based
   * on the operation name so that irrespective of the order of insertion, same set of operations always generate
   * same canonicalize form. This representation is then used for computing the checksum. So if there are any changes
   * to this representation, upgrade step would be required to update all the checksums stored in store.
   */
  private String canonicalize() {
    List<Operation> ops = new ArrayList<>(operations);
    ops.sort(Comparator.comparing(Operation::getName));
    return GSON.toJson(ops);
  }

  private static final long EMPTY64 = 0xc15d213aa4d7a795L;

  /**
   * The implementation of fingerprint algorithm is copied from {@code org.apache.avro.SchemaNormalization} class.
   *
   * @param data byte string for which fingerprint is to be computed
   * @return the 64-bit Rabin Fingerprint (as recommended in the Avro spec) of a byte string
   */
   private long fingerprint64(byte[] data) {
    long result = EMPTY64;
    for (byte b: data) {
      int index = (int) (result ^ b) & 0xff;
      result = (result >>> 8) ^ FP64.FP_TABLE[index];
    }
    return result;
  }

  /* An inner class ensures that FP_TABLE initialized only when needed. */
  private static class FP64 {
    private static final long[] FP_TABLE = new long[256];
    static {
      for (int i = 0; i < 256; i++) {
        long fp = i;
        for (int j = 0; j < 8; j++) {
          long mask = -(fp & 1L);
          fp = (fp >>> 1) ^ (EMPTY64 & mask);
        }
        FP_TABLE[i] = fp;
      }
    }
  }

  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }

    if (!(o instanceof FieldLineageInfo)) {
      return false;
    }

    FieldLineageInfo info = (FieldLineageInfo) o;
    return checksum == info.checksum;
  }


  @Override
  public int hashCode() {
    return (int) (checksum ^ (checksum >>> 32));
  }
}
